{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Text Classification (Sentiment Analysis) Using Bayes Rule \n",
    "==============\n",
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "Your goal in this part of assigment is to implement a Naive Bayes Multinomial classifier using  bag of words model for the classification of text (movie reviews) into different categories..\n",
    "\n",
    "**Note** Please note that you are allowed to use only those libraries which we have discussed in the class, i.e. numpy, scipy, pandas.\n",
    "\n",
    "Once you have build and test the model on the provided dataset. You will use the learned techniques to compete in a [Kaggle](https://www.kaggle.com/c/word2vec-nlp-tutorial) competition and report your final score and leaderboard ranking to get full credit.\n",
    "\n",
    "For final submission attach the screen-shot of the leader-board with your score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import scipy.stats\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "checksum": "41bc7183ffc40d65a148fc72fb955046",
     "grade": false,
     "grade_id": "parse_string",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['computer', 'scientists', 'are', 'the', 'rockstars', 'of', 'tomorrow', 'cool']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import bs4\n",
    "\n",
    "def parse_string(string): \n",
    "    \"\"\"\"\n",
    "        Parse the input string and tokenize it using regular expressisons:\n",
    "        First clean the string such that it does not have any punctuation or number, it must only have a-z and A-Z.\n",
    "        Please note that while doing this, the spaces much not get disturbed, but in case of multiple spaces convert \n",
    "        them to one space.\n",
    "        Then convert the string to lower case and return its words as a list of strings.\n",
    "        \n",
    "        Example:\n",
    "        --------\n",
    "        Input :  computer scien_tist-s are,,,  the  rock__stars of tomorrow_ <cool>  ????\n",
    "        Output:  ['computer', 'scientists', 'are', 'the', 'rockstars', 'of', 'tomorrow']\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        string: string to be parsed...\n",
    "        re: regular expression to be used for the tokenization.\n",
    "        \n",
    "        Returns:\n",
    "        ---------\n",
    "        list of tokens extracted from the string...\n",
    "    \"\"\"\n",
    "    \n",
    "#     newstring= string.split()\n",
    "    \n",
    "    # removing punctuation and numbers\n",
    "    newstring=re.sub(r'[^a-zA-Z\\s]', '', string).replace(\"_\",\"\")\n",
    "    newstring=newstring.lower()\n",
    "    \n",
    "    newstring=newstring.split()\n",
    "    newstring= list(filter(None, newstring))\n",
    "    \n",
    "    return newstring    \n",
    "    \n",
    "parse_string(\"computer scien_tist-s are,,,  the  rock__stars of tomorrow_ 123<cool>  ????\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a5035e3a9d5a9db47fdbafa62cc8a6a3",
     "grade": false,
     "grade_id": "parse_file",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_file(filename): # Parse a given file\n",
    "    \"\"\"\"\n",
    "        Parameters:\n",
    "        ----------\n",
    "        filename: name of text file to be read\n",
    "   \n",
    "        \n",
    "        Returns:\n",
    "        ---------\n",
    "        read file as raw string (with \\n, \\t, \\r, etc included)\n",
    "    \"\"\"\n",
    "    \n",
    "    text=None\n",
    "\n",
    "    with open(filename,'r',encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "checksum": "6d2b49b37e798724777c6c429add90f1",
     "grade": false,
     "grade_id": "files_to_string",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def files_to_strings(X):\n",
    "    \n",
    "    \"\"\"\n",
    "        Read an array (or list) of files where each file content is read in a string...\n",
    "        Input:\n",
    "        -------\n",
    "        X an array (or list) of file names\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X as a numpy array with each row containing a read string from the file...\n",
    "    \"\"\"\n",
    "    \n",
    "    file_content=[]\n",
    "    for i in X:\n",
    "        content= parse_file(i)\n",
    "        file_content.append(content)\n",
    "    \n",
    "    file_content=np.array(file_content)\n",
    "    return file_content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "96714a19700e62e9b60de218a6e81b58",
     "grade": true,
     "grade_id": "test_reading_and_parsing",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal, assert_list_equal\n",
    "\n",
    "assert_list_equal(parse_string(\"computer scien_tist-s are,,,  the  rock__stars of tomorrow_ <cool>  ????\"),\n",
    "        [u'computer', u'scientists', u'are', u'the', u'rockstars', u'of', u'tomorrow', u'cool'], \"Incorrect cleanning\")\n",
    "\n",
    "\n",
    "strings = files_to_strings(np.array([\"./data/imdb1/neg/cv000_29416.txt\", \"./data/imdb1/pos/cv000_29590.txt\"]))\n",
    "with open(\"./data/imdb1/neg/cv000_29416.txt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "assert_equal(strings[0], text, \"At first index should be text of first file\")\n",
    "assert_equal(strings.shape, (2,), \"Shape must be (2,) for two files in list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removestopwords(listofwords):\n",
    "    stopword=set(t.read_txt_file(r'./data/english.stop'))\n",
    "    \n",
    "    filtered_sentence = [word for word in listofwords if word.lower() not in stopword]\n",
    "    \n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4412e1df0ae5399bde4cfc7440cdc052",
     "grade": false,
     "grade_id": "classifier",
     "locked": false,
     "solution": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    ''' Implements the Naive Bayes For Text Classification... '''\n",
    "    def __init__(self, classes):\n",
    "        self.classes = classes  # e.g., ['neg', 'pos']\n",
    "        self.priors = {}  # Prior probabilities for each class\n",
    "        self.word_to_id = {}  # Unique word to ID mapping\n",
    "        self.word_count = {cls: defaultdict(int) for cls in classes}  # Word counts per class\n",
    "        self.class_count = {cls: 0 for cls in classes}  # Document counts per class\n",
    "        self.total_words = {cls: 0 for cls in classes}  # Total word counts per class\n",
    "        self.vocab_size = 0  # Vocabulary size\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        ''' Train the Naive Bayes classifier using the given data and labels. '''\n",
    "        # Preprocess the training data\n",
    "        new_X = []\n",
    "        for i in X:\n",
    "            filtered_str = parse_string(i[0])\n",
    "            filtered_stopwords = removestopwords(filtered_str)\n",
    "            new_X.append(filtered_stopwords)\n",
    "        \n",
    "        X = new_X\n",
    "        \n",
    "        # Build vocabulary\n",
    "        words = list(chain(*X))\n",
    "        unique_words = sorted(set(words))\n",
    "        self.word_to_id = {word: idx for idx, word in enumerate(unique_words, start=1)}\n",
    "        self.vocab_size = len(self.word_to_id)\n",
    "        \n",
    "        # Calculate priors and word counts\n",
    "        for cls in self.classes:\n",
    "            cls_indices = np.where(Y == cls)[0]\n",
    "            self.class_count[cls] = len(cls_indices)\n",
    "            self.priors[cls] = np.log(self.class_count[cls] / len(Y))  # Use log priors directly\n",
    "            \n",
    "            for idx in cls_indices:\n",
    "                for word in X[idx]:\n",
    "                    self.word_count[cls][word] += 1\n",
    "                    self.total_words[cls] += 1\n",
    "\n",
    "    def test(self, X):\n",
    "        ''' Test the trained classifier on the given test data. '''\n",
    "        # Preprocess the test data\n",
    "        new_X = []\n",
    "        for i in X:\n",
    "            filtered_str = parse_string(i[0])\n",
    "            filtered_stopwords = removestopwords(filtered_str)\n",
    "            new_X.append(filtered_stopwords)\n",
    "        \n",
    "        X = new_X\n",
    "        predictions = []\n",
    "        \n",
    "        for document in X:\n",
    "            log_probs = {}\n",
    "            for cls in self.classes:\n",
    "                log_probs[cls] = self.priors[cls]  # Initialize with log prior\n",
    "                for word in document:\n",
    "                    # Get word count in class with Laplace smoothing\n",
    "                    word_count = self.word_count[cls].get(word, 0)\n",
    "                    # Calculate log probability with Laplace smoothing\n",
    "                    log_prob = np.log((word_count + 1) / (self.total_words[cls] + self.vocab_size))\n",
    "                    log_probs[cls] += log_prob\n",
    "            # Assign the class with the highest log probability\n",
    "            predicted_class = max(log_probs, key=log_probs.get)\n",
    "            predictions.append(predicted_class)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        ''' Predict the class of a single input example (list of words). '''\n",
    "        return self.test([x])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tools as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tdir= \"./data/imdb1/\" # training dir...\n",
    "#load data, get list of files for each class...\n",
    "posfiles=t.get_files(tdir+'/pos','*',withpath=True)\n",
    "negfiles=t.get_files(tdir+'/neg','*',withpath=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data Dimensions = (2000,)  Training labels dimensions= (2000,)\n"
     ]
    }
   ],
   "source": [
    "#generate training and testing data...\n",
    "plabels=['pos']*len(posfiles)\n",
    "nlabels=['neg']*len(posfiles)\n",
    "labels=np.concatenate((plabels,nlabels)) # concatenate the +ve and -ve labels\n",
    "tX=np.concatenate((posfiles,negfiles))\n",
    "print (\"Training data Dimensions =\", tX.shape,\" Training labels dimensions=\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['./data/imdb1//pos\\\\cv000_29590.txt',\n",
       "       './data/imdb1//pos\\\\cv001_18431.txt',\n",
       "       './data/imdb1//pos\\\\cv002_15918.txt', ...,\n",
       "       './data/imdb1//neg\\\\cv997_5152.txt',\n",
       "       './data/imdb1//neg\\\\cv998_15691.txt',\n",
       "       './data/imdb1//neg\\\\cv999_14636.txt'], dtype='<U33')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=files_to_strings(tX) # read files and convert each file into set of strings and return an numpy array\n",
    "X = X.reshape((X.shape[0], 1))\n",
    "#Split the data into two halves training and test set...\n",
    "traindata,trainlabels,testdata,testlabels=t.split_data(X,labels)\n",
    "#Find the classes to train\n",
    "classes=np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] training a classifier for following classes neg, pos\n",
      "[Info] Accuracy = 0.81\n"
     ]
    }
   ],
   "source": [
    "#Now build a Naive Bayes classifier and test it...\n",
    "print ('[Info] training a classifier for following classes {}, {}'.format(classes[0],classes[1]))\n",
    "nb=NaiveBayes(classes)\n",
    "nb.train(traindata,trainlabels)\n",
    "pclasses=nb.test(testdata)\n",
    "acc=np.sum(pclasses==testlabels)/float(testlabels.shape[0])\n",
    "print (\"[Info] Accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cells Start\n",
    "#### Do not Modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5822a32f567e3fb7e7f8515eacddec7d",
     "grade": true,
     "grade_id": "test_classifier_shapes",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_in\n",
    "\n",
    "nb=NaiveBayes(classes)\n",
    "nb.train(traindata,trainlabels)\n",
    "assert_equal (nb.test(testdata).shape[0], testdata.shape[0])\n",
    "assert_in( type(nb.predict([\"ok\"])) , [str, np.string_, np.str, np.str_] , \"Predict should return a label \\\n",
    "                                                                                            not list or array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c4695c7d2b42d0e0570c50ab36361afd",
     "grade": true,
     "grade_id": "test_acc",
     "locked": true,
     "points": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_greater\n",
    "\n",
    "nb=NaiveBayes(classes)\n",
    "nb.train(traindata,trainlabels)\n",
    "pclasses=nb.test(testdata)\n",
    "acc=np.sum(pclasses==testlabels)/float(testlabels.shape[0])\n",
    "assert_greater(acc, 0.77, \"Acc must be greater then 77% you are doing something wrong\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "268244e33988983787a2f3e3b8f6eba3",
     "grade": true,
     "grade_id": "test_classifier_responce",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "comment_pos = \"A nice movie, the case was good. Overall a perfect play\"\n",
    "comment_neg = \"A waste of time, cast was bad. a clear No!\"\n",
    "\n",
    "#generate training and testing data...\n",
    "tX=np.concatenate((posfiles,negfiles))\n",
    "X=files_to_strings(tX)\n",
    "X = X.reshape((X.shape[0], 1))\n",
    "\n",
    "plabels=['pos']*len(posfiles)\n",
    "nlabels=['neg']*len(posfiles)\n",
    "true_labels = np.concatenate((plabels,nlabels))\n",
    "inverted_labels = np.concatenate((nlabels,plabels))\n",
    "\n",
    "true_nb=NaiveBayes(classes)\n",
    "true_nb.train(X,true_labels)\n",
    "\n",
    "inverted_nb=NaiveBayes(classes)\n",
    "inverted_nb.train(X,inverted_labels)\n",
    "\n",
    "assert_equal( true_nb.predict(comment_pos.split()), \"pos\" )\n",
    "assert_equal( true_nb.predict(comment_neg.split()), \"neg\" )\n",
    "\n",
    "assert_equal( inverted_nb.predict(comment_pos.split()), \"neg\" )\n",
    "assert_equal( inverted_nb.predict(comment_neg.split()), \"pos\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cells End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "Now lets throw our methods to winds of different folds and measure their accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating CV data for 2 classes\n",
      "(1800, 1) (200, 1)\n",
      "(1800, 1) (200, 1)\n",
      "(1800, 1) (200, 1)\n",
      "(1800, 1) (200, 1)\n",
      "(1800, 1) (200, 1)\n",
      "(1800, 1) (200, 1)\n",
      "(1800, 1) (200, 1)\n",
      "(1800, 1) (200, 1)\n",
      "(1800, 1) (200, 1)\n",
      "(1800, 1) (200, 1)\n"
     ]
    }
   ],
   "source": [
    "#Now lets generate n-fold training and testing data...\n",
    "nfolds=10\n",
    "folds=t.generate_folds(X,labels,nfolds) # generate folds for \n",
    "for k in arange(len(folds)):\n",
    "    print (folds[k][0].shape, folds[k][2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Fold 1 Accuracy = 0.825\n",
      "[Info] Fold 2 Accuracy = 0.81\n",
      "[Info] Fold 3 Accuracy = 0.81\n",
      "[Info] Fold 4 Accuracy = 0.835\n",
      "[Info] Fold 5 Accuracy = 0.82\n",
      "[Info] Fold 6 Accuracy = 0.805\n",
      "[Info] Fold 7 Accuracy = 0.78\n",
      "[Info] Fold 8 Accuracy = 0.815\n",
      "[Info] Fold 9 Accuracy = 0.805\n",
      "[Info] Fold 10 Accuracy = 0.825\n",
      "[0.825, 0.81, 0.81, 0.835, 0.82, 0.805, 0.78, 0.815, 0.805, 0.825]\n",
      "[Info] Mean Accuracy = 0.813\n"
     ]
    }
   ],
   "source": [
    "totacc=[]\n",
    "#train a classifier for each fold...\n",
    "classes=np.unique(labels)\n",
    "\n",
    "for k in range(nfolds):\n",
    "    nb=NaiveBayes(classes)\n",
    "    \n",
    "    traindata=folds[k][0]\n",
    "    trainlabels=folds[k][1]\n",
    "    \n",
    "    #Lets first train the classifier\n",
    "    nb.train(traindata,trainlabels)\n",
    "    \n",
    "    testdata=folds[k][2]\n",
    "    testlabels=folds[k][3]\n",
    "    \n",
    "    #Lets test the classifier\n",
    "    pclasses= nb.test(testdata)\n",
    "    \n",
    "    #print pclasses\n",
    "    acc=np.sum(pclasses==testlabels)/float(testlabels.shape[0])\n",
    "    print (\"[Info] Fold {} Accuracy = {}\".format(k+1, acc))\n",
    "    \n",
    "    totacc.append(acc)\n",
    "\n",
    "print (totacc)\n",
    "\n",
    "mean_acc = np.mean(totacc)\n",
    "print ('[Info] Mean Accuracy =', mean_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Excellent, now its time to go into real waters of Kaggle.\n",
    "\n",
    "\n",
    "You will be needed to create an account on the Kaggle and download the data for the competition [\"Bag of words meets bags of popcorn\"](https://www.kaggle.com/c/word2vec-nlp-tutorial/data).  Note that you will be only downloading the \"labeledTrainData.tsv\" and \"labeledTestData.tsv\".\n",
    "\n",
    "\n",
    "\"labeledTrainData.tsv\" will be used for training your model and thus have prespecified labels for each example review. \"labeledTestData.tsv\" will be used for testing your model and thus don't have prespecified labels for each example. You will predicting the label for each review and then uploading your result to Kaggle server which will be evaluating your model and will give score to your entry. You will report this score during your assignment submission.\n",
    "\n",
    "**[Caution]** Please note that Kaggle limits maximum number of evaluations per 24 hours to 5 to reduce the overfitting on the test set, so be careful and throughly test your model before submitting your entry to Kaggle server. \n",
    "\n",
    "Read the instructions on the Competition Page. Note you are not allowed to use any of the library except what we have learned during class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data-set\n",
    "train=pd.read_csv('labeledTrainData.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.50001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment\n",
       "count  25000.00000\n",
       "mean       0.50000\n",
       "std        0.50001\n",
       "min        0.00000\n",
       "25%        0.00000\n",
       "50%        0.50000\n",
       "75%        1.00000\n",
       "max        1.00000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "Yt=train['sentiment']\n",
    "Xt=train['review']\n",
    "Xt=np.array(Xt)\n",
    "Yt=np.array(Yt)\n",
    "\n",
    "print (Xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read test set...\n",
    "test=pd.read_csv('testData.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>Naturally in a film who's main themes are of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>This movie is a disaster within a disaster fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>All in all, this is a movie for kids. We saw i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>Afraid of the Dark left me with the impression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>A very accurate depiction of small time mob li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             review\n",
       "0  12311_10  Naturally in a film who's main themes are of m...\n",
       "1    8348_2  This movie is a disaster within a disaster fil...\n",
       "2    5828_4  All in all, this is a movie for kids. We saw i...\n",
       "3    7186_2  Afraid of the Dark left me with the impression...\n",
       "4   12128_7  A very accurate depiction of small time mob li..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "560d69ba70bb62a18e317665c5058abe",
     "grade": false,
     "grade_id": "kaggle_cell_1",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's split the training data into two halves and test our accuracy...\n",
    "traindata,trainlabels,testdata,testlabels=t.split_data(Xt.reshape((Xt.shape[0],1)),Yt)\n",
    "classes=np.unique(trainlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "65200523b8bf7de2418ba0de0fb987c6",
     "grade": false,
     "grade_id": "kaggle_cell_2",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] training a classifier for following classes 0, 1\n",
      "[Info] Accuracy = 0.8564\n"
     ]
    }
   ],
   "source": [
    "# Now lets go and train the model and see its performance...\n",
    "print ('[Info] training a classifier for following classes {}, {}'.format(classes[0],classes[1]))\n",
    "nb=NaiveBayes(classes)\n",
    "nb.train(traindata,trainlabels)\n",
    "pclasses=nb.test(testdata)\n",
    "acc=np.sum(pclasses==testlabels)/float(testlabels.shape[0])\n",
    "print (\"[Info] Accuracy = {}\".format(acc) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation Time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e3f8832c8d5c5aec3a4686f9cf13bf5a",
     "grade": false,
     "grade_id": "kaggle_cell_3",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating CV data for 2 classes\n",
      "(22500, 1) (2500, 1)\n",
      "(22500, 1) (2500, 1)\n",
      "(22500, 1) (2500, 1)\n",
      "(22500, 1) (2500, 1)\n",
      "(22500, 1) (2500, 1)\n",
      "(22500, 1) (2500, 1)\n",
      "(22500, 1) (2500, 1)\n",
      "(22500, 1) (2500, 1)\n",
      "(22500, 1) (2500, 1)\n",
      "(22500, 1) (2500, 1)\n"
     ]
    }
   ],
   "source": [
    "#Split the training data into 10 folds and test classifiers performance...\n",
    "\n",
    "nfolds=10\n",
    "folds=t.generate_folds(Xt.reshape((Xt.shape[0],1)),Yt,nfolds) # generate folds for \n",
    "for k in arange(len(folds)):\n",
    "    print (folds[k][0].shape, folds[k][2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "85c038543210c9d4b02762be14730d9b",
     "grade": false,
     "grade_id": "kaggle_cell_4",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Fold 1 Accuracy = 0.8576\n",
      "[Info] Fold 2 Accuracy = 0.8776\n",
      "[Info] Fold 3 Accuracy = 0.866\n",
      "[Info] Fold 4 Accuracy = 0.8552\n",
      "[Info] Fold 5 Accuracy = 0.848\n",
      "[Info] Fold 6 Accuracy = 0.8544\n",
      "[Info] Fold 7 Accuracy = 0.8596\n",
      "[Info] Fold 8 Accuracy = 0.85\n",
      "[Info] Fold 9 Accuracy = 0.8516\n",
      "[Info] Fold 10 Accuracy = 0.8436\n",
      "[0.8576, 0.8776, 0.866, 0.8552, 0.848, 0.8544, 0.8596, 0.85, 0.8516, 0.8436]\n",
      "[Info] Mean Accuracy = 0.8563600000000001\n"
     ]
    }
   ],
   "source": [
    "# As it takes time, so becareful it can cause your machine into red hot oven\n",
    "totacc=[]\n",
    "classes=np.unique(Yt)\n",
    "\n",
    "for k in range(nfolds):\n",
    "    nb=NaiveBayes(classes)\n",
    "    \n",
    "    traindata=folds[k][0]\n",
    "    trainlabels=folds[k][1]\n",
    "    \n",
    "    #Lets first train the classifier\n",
    "    nb.train(traindata,trainlabels)\n",
    "    \n",
    "    testdata=folds[k][2]\n",
    "    testlabels=folds[k][3]\n",
    "    \n",
    "    #Lets test the classifier\n",
    "    pclasses= nb.test(testdata)\n",
    "    \n",
    "    acc=np.sum(pclasses==testlabels)/float(testlabels.shape[0])\n",
    "    print (\"[Info] Fold {} Accuracy = {}\".format(k+1, acc) ) \n",
    "    \n",
    "    totacc.append(acc)\n",
    "\n",
    "print (totacc)\n",
    "print ('[Info] Mean Accuracy =', np.mean(totacc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's train on the complete dataset and test on the provided test set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c27687a3a83576b5360a0e383909305f",
     "grade": false,
     "grade_id": "kaggle_cell_5",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a Classifier on Full training set with classes = [0 1]\n"
     ]
    }
   ],
   "source": [
    "classes= np.unique(Yt)\n",
    "print ('Training a Classifier on Full training set with classes =', classes)\n",
    "nb=NaiveBayes(classes)\n",
    "nb.train(Xt.reshape(Xt.shape[0],1),Yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "checksum": "2df4444c4b4fcbde8568ffea728d5658",
     "grade": false,
     "grade_id": "kaggle_cell_6",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the test data...\n",
    "Xtest = test['review'].values  # Convert the Series to a NumPy array directly\n",
    "\n",
    "# Reshape the array if needed\n",
    "Xtest = Xtest.reshape((Xtest.shape[0], 1))\n",
    "\n",
    "# Test the classifier on the provided test set...\n",
    "pclasses = nb.test(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "checksum": "a902480b291b88f6c9fb7422cd845305",
     "grade": false,
     "grade_id": "kaggle_cell_7",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#write the result in the kaggle's required format\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":pclasses} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"Naive_bays_Bag_of_Words_model.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to Upload the prediction to Kaggle...\n",
    "\n",
    "Now upload the result on the Kaggle and see your ranking and score. Using this simple method you can have an accuracy of around 0.80960."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement by Excluding Stop Words...\n",
    "\n",
    "You can improve your score further by excluding the commonly occuring words (also known as stop words) in the English language.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read and create a set of stop \n",
    "stopwords=set(t.read_txt_file('./data/sentiment/data/english.stop'))\n",
    "print (stopwords)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
