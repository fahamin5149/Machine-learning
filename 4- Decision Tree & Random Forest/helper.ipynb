{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc3f014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(x1, x2):\n",
    "    return -(x1*np.log2(x1+1e-9)+ x2*np.log2(x2+1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09b0b8c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 56\u001b[0m\n\u001b[0;32m     52\u001b[0m                 split_point_entropy\u001b[38;5;241m=\u001b[39m ((\u001b[38;5;28mlen\u001b[39m(left)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(Y))\u001b[38;5;241m*\u001b[39m(entropy_left)) \u001b[38;5;241m+\u001b[39m ((\u001b[38;5;28mlen\u001b[39m(right)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(Y))\u001b[38;5;241m*\u001b[39m(entropy_right))\n\u001b[0;32m     54\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(split_point_entropy)\n\u001b[1;32m---> 56\u001b[0m calculate_splits(X, Y)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "def calculate_splits(X, Y):\n",
    "    nexamples, nfeatures=X.shape\n",
    "\n",
    "    # CALCULATING PRIOR PROBABILITY\n",
    "\n",
    "    labels= np.unique(Y)\n",
    "    \n",
    "    c1_len= len(Y[Y==labels[0]]) #setosa\n",
    "    c2_len= len(Y[Y==labels[1]]) #versicolor\n",
    "\n",
    "    prob_c1= c1_len/len(Y) #setosa \n",
    "    prob_c2= c2_len/len(Y) #versicolor\n",
    "\n",
    "\n",
    "    # CALCULATING THE SPLIT ENTROPY OF THE ENTIRE DATA\n",
    "\n",
    "    H_D= -(prob_c1*np.log2(prob_c1+1e-9)+ prob_c2*np.log2(prob_c2+1e-9))\n",
    "\n",
    "    # Loop till the nimber of columns to find the possible splits for each feature\n",
    "    for i in range(X.shape[1]):\n",
    "        x_data= X[:,i]\n",
    "\n",
    "        unique_vals= np.unique(x_data)\n",
    "\n",
    "        # GETTING THE MIDPOINT OF UNIQUE VALS TO GET THE SPLIT POINTS\n",
    "        possible_splits = [(unique_vals[i]+unique_vals[i+1])/2 for i in range(len(unique_vals) - 1)]\n",
    "\n",
    "        for j in possible_splits:\n",
    "            leftchilds= x_data<= j # data<=split point [true, false, true] as masked result\n",
    "            rightchilds= x_data>j # data>split point [true, false, true] as masked result\n",
    "\n",
    "            if np.any(leftchilds) and np.any(rightchilds):  # Ensure both sides have samples\n",
    "                \n",
    "                left= Y[leftchilds] # applying the mask to Y to get left labels (setosa, virginica, setosa) \n",
    "                right= Y[rightchilds] # applying the mask to Y to get right labels (setosa, virginica, setosa)\n",
    "\n",
    "                left_c1= left[left==labels[0]] # getting class 1 from left\n",
    "                left_c2= left[left==labels[1]] # getting class 2 from left\n",
    "\n",
    "                # PROBABILITY OF CLASS1/2 GIVEN LEFT CHILD \n",
    "                prob_c1_left= len(left_c1)/(len(left_c1)+len(left_c2))\n",
    "                prob_c2_left= len(left_c2)/(len(left_c1)+len(left_c2))\n",
    "                \n",
    "                # PROBABILITY OF CLASS1/2 GIVEN LEFT CHILD \n",
    "                prob_c1_right = (c1_len - len(left_c1)) / ((c1_len - len(left_c1)) + (c2_len - len(left_c2)))\n",
    "                prob_c2_right = (c2_len - len(left_c2)) / ((c1_len - len(left_c1)) + (c2_len - len(left_c2)))\n",
    "\n",
    "                # ENTROPY OF LEFT CHILD/ RIGHT CHILD\n",
    "                entropy_left=calculate_entropy(prob_c1_left,prob_c2_left)\n",
    "                entropy_right=calculate_entropy(prob_c1_right, prob_c2_right)\n",
    "                \n",
    "                split_point_entropy= ((len(left)/len(Y))*(entropy_left)) + ((len(right)/len(Y))*(entropy_right))\n",
    "                \n",
    "                print(split_point_entropy)\n",
    "                \n",
    "calculate_splits(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(x1, x2):\n",
    "    return -(x1*np.log2(x1+1e-9)+ x2*np.log2(x2+1e-9))\n",
    "\n",
    "def calculate_splits(X, Y):\n",
    "    nexamples, nfeatures=X.shape\n",
    "\n",
    "    # CALCULATING PRIOR PROBABILITY\n",
    "\n",
    "    labels= np.unique(Y)\n",
    "    \n",
    "    c1_len= len(Y[Y==labels[0]]) #setosa\n",
    "    c2_len= len(Y[Y==labels[1]]) #versicolor\n",
    "\n",
    "    prob_c1= c1_len/len(Y) #setosa \n",
    "    prob_c2= c2_len/len(Y) #versicolor\n",
    "\n",
    "\n",
    "    # CALCULATING THE SPLIT ENTROPY OF THE ENTIRE DATA\n",
    "\n",
    "    H_D= -(prob_c1*np.log2(prob_c1+1e-9)+ prob_c2*np.log2(prob_c2+1e-9))\n",
    "\n",
    "    # Loop till the nimber of columns to find the possible splits for each feature\n",
    "    for i in range(X.shape[1]):\n",
    "        x_data= X[:,i]\n",
    "\n",
    "        unique_vals= np.unique(x_data)\n",
    "\n",
    "        # GETTING THE MIDPOINT OF UNIQUE VALS TO GET THE SPLIT POINTS\n",
    "        possible_splits = [(unique_vals[i]+unique_vals[i+1])/2 for i in range(len(unique_vals) - 1)]\n",
    "\n",
    "        for j in possible_splits:\n",
    "            leftchilds= x_data<= j # data<=split point [true, false, true] as masked result\n",
    "            rightchilds= x_data>j # data>split point [true, false, true] as masked result\n",
    "\n",
    "            if np.any(leftchilds) and np.any(rightchilds):  # Ensure both sides have samples\n",
    "                \n",
    "                left= Y[leftchilds] # applying the mask to Y to get left labels (setosa, virginica, setosa) \n",
    "                right= Y[rightchilds] # applying the mask to Y to get right labels (setosa, virginica, setosa)\n",
    "\n",
    "                left_c1= left[left==labels[0]] # getting class 1 from left\n",
    "                left_c2= left[left==labels[1]] # getting class 2 from left\n",
    "\n",
    "                # PROBABILITY OF CLASS1/2 GIVEN LEFT CHILD \n",
    "                prob_c1_left= len(left_c1)/(len(left_c1)+len(left_c2))\n",
    "                prob_c2_left= len(left_c2)/(len(left_c1)+len(left_c2))\n",
    "                \n",
    "                # PROBABILITY OF CLASS1/2 GIVEN LEFT CHILD \n",
    "                prob_c1_right = (c1_len - len(left_c1)) / ((c1_len - len(left_c1)) + (c2_len - len(left_c2)))\n",
    "                prob_c2_right = (c2_len - len(left_c2)) / ((c1_len - len(left_c1)) + (c2_len - len(left_c2)))\n",
    "\n",
    "                # ENTROPY OF LEFT CHILD/ RIGHT CHILD\n",
    "                entropy_left=calculate_entropy(prob_c1_left,prob_c2_left)\n",
    "                entropy_right=calculate_entropy(prob_c1_right, prob_c2_right)\n",
    "                \n",
    "                split_point_entropy= ((len(left)/len(Y))*(entropy_left)) + ((len(right)/len(Y))*(entropy_right))\n",
    "                \n",
    "                print(split_point_entropy)\n",
    "                \n",
    "calculate_splits(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb695d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "## Your code goes here...\n",
    "# You might need to define auxliary classes for composition.. ?\n",
    "class DecisionTree:\n",
    "    ''' Implements the Decision Tree For Classification... '''\n",
    "    def __init__(self, purityp, exthreshold,maxdepth=10): #0.95, 5,2       \n",
    "        self.purity=purityp\n",
    "        '''\n",
    "        This parameter sets a threshold for the minimum number of examples (data points) a node must have to be split \n",
    "        further. If a node has fewer examples than exthreshold, it becomes a leaf node, and the tree wonâ€™t attempt to \n",
    "        split it any further.\n",
    "        '''\n",
    "        self.exthreshold=exthreshold\n",
    "        self.maxdepth=maxdepth\n",
    "        \n",
    "    def calculate_purity(self, Y):\n",
    "        \"\"\"\n",
    "        Calculates the purity of the current node based on the distribution of labels in Y.\n",
    "        \n",
    "        Purity here is measured by the fraction of the most common label in Y.\n",
    "        \"\"\"\n",
    "        \n",
    "        counts = Counter(Y) # {'Iris-setosa': 3, 'Iris-virginica': 2}\n",
    "        max_label = max(label_counts.values()) # 3\n",
    "\n",
    "        # Step 3: Calculate purity\n",
    "        purity = max_label / len(Y)\n",
    "        \n",
    "        return purity\n",
    "        \n",
    "    def train(self, X, Y):\n",
    "        ''' Train Decision Tree using the given \n",
    "            X [m x d] data matrix and Y labels matrix\n",
    "            \n",
    "            Input:\n",
    "            ------\n",
    "            X: [m x d] a data matrix of m d-dimensional examples.\n",
    "            Y: [m x 1] a label vector.\n",
    "            \n",
    "            Returns:\n",
    "            -----------\n",
    "            Nothing\n",
    "            '''\n",
    "        nexamples,nfeatures=X.shape\n",
    "        self.klasses=np.unique(Y)\n",
    "        \n",
    "        self.tree = self.build_tree(X, Y, depth=0)\n",
    "        \n",
    "        \n",
    "    def build_tree(self, X, Y, depth):\n",
    "        \"\"\" \n",
    "            Function is used to recursively build the decision Tree \n",
    "          \n",
    "            Input\n",
    "            -----\n",
    "            X: [m x d] a data matrix of m d-dimensional examples.\n",
    "            Y: [m x 1] a label vector.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            root node of the built tree...\n",
    "        \"\"\"\n",
    "        nexamples, nfeatures=X.shape\n",
    "      \n",
    "        klasses=np.unique(Y);\n",
    "        print(\"Number of classes: \",len(klasses))\n",
    "        current_purity = self.calculate_purity(Y)\n",
    "        \n",
    "        # IF NUMBER OF CLASS IN A NODE IS 1 MEANING PURE LABEL\n",
    "        # IF PURITY THRESHOLD IS MET >0.95\n",
    "        # IF MAX DEPT IS MET IE 10\n",
    "        # THE MINIMUM NUMBER OF EXAMPLE IN A NODE IS MET\n",
    "        if len(klasses)==1 or current_purity>=self.purity or depth >= self.maxdepth or nexamples <= self.exthreshold:\n",
    "            label_counts = Counter(Y)\n",
    "\n",
    "            # Step 1: Determine the label with the maximum count\n",
    "            final_label = max(label_counts, key=label_counts.get)\n",
    "\n",
    "            # Step 2: Calculate purity as the maximum count divided by total examples\n",
    "            max_count = label_counts[final_label]\n",
    "            purity = max_count / len(Y)\n",
    "\n",
    "        \n",
    "            return Node(purity, klasslabel=final_label)\n",
    "            \n",
    "        \n",
    "        Node_best_score = float('inf')  # We want to minimize entropy\n",
    "        Node_best_split = None\n",
    "        Node_Xlidx, Node_Xridx = None, None\n",
    "        feature_index=None\n",
    "        # GETTING THE BEST SPLIT POINT\n",
    "        \n",
    "        for i in range(nfeatures):\n",
    "            print(len(X[:,i]),len(Y))\n",
    "            best_split, best_score, Xlidx, Xridx= self.evaluate_numerical_attribute(X[:,i],Y)\n",
    "            #print(best_split, best_score, Xlidx, Xridx)\n",
    "            \n",
    "            if best_score< Node_best_score:\n",
    "                feature_index=i\n",
    "                Node_best_split = best_split\n",
    "                Node_best_score = best_score\n",
    "                Node_Xlidx, Node_Xridx = Xlidx, Xridx\n",
    "          \n",
    "        \"\"\"\n",
    "            def __init__(self,purity,klasslabel='',score=0,split=[],fidx=-1):\n",
    "            self.lchild=None       \n",
    "            self.rchild=None\n",
    "            self.klasslabel=klasslabel        \n",
    "            self.split=split\n",
    "            self.score=score\n",
    "            self.fidx=fidx #feature index\n",
    "            self.purity=purity\n",
    "        \"\"\"\n",
    "        print(\"The best root node is\\n \",Node_best_split, Node_best_score, Node_Xlidx, Node_Xridx)\n",
    "        \n",
    "        # If no valid split, return a leaf node\n",
    "        if feature_index is None:\n",
    "            \n",
    "            \n",
    "            label_counts = Counter(Y)\n",
    "\n",
    "            # Step 1: Determine the label with the maximum count\n",
    "            final_label = max(label_counts, key=label_counts.get)\n",
    "\n",
    "            # Step 2: Calculate purity as the maximum count divided by total examples\n",
    "            max_count = label_counts[final_label]\n",
    "            purity = max_count / len(Y)\n",
    "        \n",
    "            return Node(purity, klasslabel=final_label)\n",
    "        \n",
    "        node=Node(self.purity,Node_best_score,[Node_best_split],feature_index)\n",
    "                \n",
    "        \"\"\"\n",
    "        X_left = X[Xlidx]  # X_left = [ [1.5], [2.5], [3.0] ]\n",
    "        Y_left = Y[Xlidx]  # Y_left = [ 0,     0,     1     ]\n",
    "\n",
    "        X_right = X[Xridx] # X_right = [ [3.5], [4.5], [5.5] ]\n",
    "        Y_right = Y[Xridx] # Y_right = [ 1,     1,     0     ]\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        X_left, Y_left = X[Node_Xlidx], Y[Node_Xlidx]\n",
    "        X_right, Y_right = X[Node_Xridx], Y[Node_Xridx]\n",
    "            \n",
    "        node.set_childs(self.build_tree(X_left, Y_left, depth + 1),\n",
    "                        self.build_tree(X_right, Y_right, depth + 1))\n",
    "        \n",
    "        \n",
    "        return node\n",
    "        \n",
    "    def test(self, X):\n",
    "        \n",
    "        ''' Test the trained classifiers on the given set of examples \n",
    "        \n",
    "                   \n",
    "            Input:\n",
    "            ------\n",
    "            X: [m x d] a data matrix of m d-dimensional test examples.\n",
    "           \n",
    "            Returns:\n",
    "            -----------\n",
    "                pclass: the predicted class for each example, i.e. to which it belongs\n",
    "        '''\n",
    "        \n",
    "        nexamples, nfeatures=X.shape\n",
    "        pclasses=self.predict(X)\n",
    "                \n",
    "        return np.array(pclasses)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class for each example in the given data matrix X.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            [m x d] data matrix of m d-dimensional test examples.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            Predicted classes for each example in X.\n",
    "        \"\"\"\n",
    "        predictions = [self._predict(self.tree, X[i, :]) for i in range(X.shape[0])]\n",
    "        return predictions\n",
    "    \n",
    "    def _predict(self, node, X):\n",
    "        \"\"\"\n",
    "        Recursively predict the class for a single example by traversing the tree.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        node : Node\n",
    "            The current node in the decision tree.\n",
    "        X : numpy.ndarray\n",
    "            A single test example (1 x d).\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        int\n",
    "            Predicted class for the given example.\n",
    "        \"\"\"\n",
    "        # If the node is a leaf, return its class label\n",
    "        if node.isleaf():\n",
    "            return node.klasslabel\n",
    "\n",
    "        # Traverse the tree according to the split condition\n",
    "        if node.isless_than_eq(X):\n",
    "            return self._predict(node.lchild, X)  # Go to left child\n",
    "        else:\n",
    "            return self._predict(node.rchild, X)  # Go to right child\n",
    "\n",
    "    def __str__(self):\n",
    "        \n",
    "        return self.__print(self.tree)\n",
    "     \n",
    "    def find_depth(self):\n",
    "        return self._find_depth(self.tree)\n",
    "    \n",
    "    \n",
    "    def _find_depth(self,node):\n",
    "        if not node:\n",
    "            return\n",
    "        if node.isleaf():\n",
    "            return 1\n",
    "        else:\n",
    "            return max(self._find_depth(node.lchild),self._find_depth(node.rchild))+1\n",
    "\n",
    "    def __print(self,node,depth=0):\n",
    "        \n",
    "        ret = \"\"\n",
    "\n",
    "        # Print right branch\n",
    "        if node.rchild:\n",
    "            ret += self.__print(node.rchild,depth+1)\n",
    "\n",
    "        # Print own value\n",
    "        \n",
    "        ret += \"\\n\" + (\"    \"*depth) + node.get_str()\n",
    "\n",
    "        # Print left branch\n",
    "        if node.lchild:\n",
    "            ret += self.__print(node.lchild,depth+1)\n",
    "        \n",
    "        return ret\n",
    "\n",
    "    def evaluate_numerical_attribute(self,feat, Y):\n",
    "        '''\n",
    "            Evaluates the numerical attribute for all possible split points for\n",
    "            possible feature selection\n",
    "            \n",
    "            Input:\n",
    "            ---------\n",
    "            feat: a contiuous feature\n",
    "            Y: labels\n",
    "            \n",
    "            Returns:\n",
    "            ----------\n",
    "            v: splitting threshold\n",
    "            score: splitting score\n",
    "            Xlidx: Index of examples belonging to left child node\n",
    "            Xridx: Index of examples belonging to right child node\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        best_score = float('inf')  # We want to minimize entropy\n",
    "        best_split = None\n",
    "        Xlidx, Xridx = None, None\n",
    "                \n",
    "        # print(classes)\n",
    "        nclasses=len(np.unique(Y))\n",
    "        \n",
    "        sidx=np.argsort(feat)\n",
    "        \n",
    "        # print(feat) Feature->X continous values \n",
    "        \n",
    "        f=feat[sidx] # sorted features\n",
    "        sY=Y[sidx] # sorted features class labels...\n",
    "        \n",
    "        # print(list(zip(f,sY))) f->new X sY->New Y\n",
    "        \n",
    "        labels = np.unique(sY)\n",
    "\n",
    "        c1_len = len(sY[sY == labels[0]])  # setosa\n",
    "        c2_len = len(sY[sY == labels[1]])  # versicolor\n",
    "\n",
    "        # CALCULATING PRIOR PROBABILITY\n",
    "        prob_c1 = c1_len / len(sY)  # setosa \n",
    "        prob_c2 = c2_len / len(sY)  # versicolor\n",
    "        \n",
    "        #print(f\"\\nPrior probabilies\\nSetosa: {prob_c1}\\nVersicolor: {prob_c2}\")\n",
    "        \n",
    "        # print(prob_c1, prob_c2)\n",
    "\n",
    "        # CALCULATING THE SPLIT ENTROPY OF THE ENTIRE DATA\n",
    "        H_D = -(prob_c1 * np.log2(prob_c1 + 1e-9) + prob_c2 * np.log2(prob_c2 + 1e-9))\n",
    "        \n",
    "        #print(f\"\\nThe split entropy of the entire data is {H_D}\")\n",
    "        # FINDING THE SPLIT ENTROPY FOR THE GIVEN FEATURE\n",
    "        x_data = f\n",
    "\n",
    "        unique_vals = np.unique(x_data)\n",
    "\n",
    "        # GETTING THE MIDPOINT OF UNIQUE VALS TO GET THE SPLIT POINTS\n",
    "        possible_splits = [(unique_vals[i] + unique_vals[i + 1]) / 2 for i in range(len(unique_vals) - 1)]\n",
    "        \n",
    "        #print(f\"\\nThe possible split point for the feature is {len(possible_splits)}\")\n",
    "\n",
    "        for j in possible_splits:\n",
    "            leftchilds = x_data <= j  # data <= split point [true, false, true] as masked result\n",
    "            rightchilds = x_data > j  # data > split point [true, false, true] as masked result\n",
    "\n",
    "            if np.any(leftchilds) and np.any(rightchilds):  # Ensure both sides have samples\n",
    "\n",
    "                left = sY[leftchilds]  # applying the mask to sY to get left labels\n",
    "                right = sY[rightchilds]  # applying the mask to sY to get right labels\n",
    "\n",
    "                #print(f\"\\nLeft Child: {left}\\nRight child:{right}\")\n",
    "                \n",
    "                left_c1 = left[left == labels[0]]  # getting class 1 from left\n",
    "                left_c2 = left[left == labels[1]]  # getting class 2 from left\n",
    "\n",
    "                # PROBABILITY OF CLASS1/2 GIVEN LEFT CHILD \n",
    "                prob_c1_left = len(left_c1) / (len(left_c1) + len(left_c2))\n",
    "                prob_c2_left = len(left_c2) / (len(left_c1) + len(left_c2))\n",
    "\n",
    "                # PROBABILITY OF CLASS1/2 GIVEN RIGHT CHILD \n",
    "                prob_c1_right = (c1_len - len(left_c1)) / ((c1_len - len(left_c1)) + (c2_len - len(left_c2)))\n",
    "                prob_c2_right = (c2_len - len(left_c2)) / ((c1_len - len(left_c1)) + (c2_len - len(left_c2)))\n",
    "\n",
    "                # ENTROPY OF LEFT CHILD/ RIGHT CHILD\n",
    "                entropy_left = calculate_entropy(prob_c1_left, prob_c2_left)\n",
    "                entropy_right = calculate_entropy(prob_c1_right, prob_c2_right)\n",
    "\n",
    "                split_point_entropy = ((len(left) / len(sY)) * entropy_left) + ((len(right) / len(sY)) * entropy_right)\n",
    "\n",
    "                #print(f\"\\n{split_point_entropy}\\n\\n******************New Split point******************\")\n",
    "                \n",
    "                \n",
    "                if split_point_entropy < best_score:\n",
    "                    best_score = split_point_entropy\n",
    "                    best_split = j\n",
    "                    Xlidx = np.where(leftchilds)[0]\n",
    "                    Xridx = np.where(rightchilds)[0]\n",
    "\n",
    "        return best_split, best_score, Xlidx, Xridx\n",
    "\n",
    "\n",
    "# print (len(Y))\n",
    "# feat=[0,1]\n",
    "# dt=DecisionTree(0.95,5,2) # def __init__(self, purityp, exthreshold,maxdepth=10)\n",
    "# feat=[0,1]\n",
    "# dt.classes=np.unique(Y)\n",
    "\n",
    "\n",
    "# dt.nclasses=len(np.unique(Y))\n",
    "# dt.train(X,Y)\n",
    "# # dt.evaluate_numerical_attribute(X[:,0],Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
